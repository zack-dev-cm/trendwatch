{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6SSO0fC1M2sVebqMFDhat",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zack-dev-cm/trendwatch/blob/main/trendwatch_yt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline"
      ],
      "metadata": {
        "id": "Ej8gh3hEnqMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YT trendwatch"
      ],
      "metadata": {
        "id": "dDZ0rMEtnsz9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-OEn__4lhZ-",
        "outputId": "76419696-c6df-4b96-cf7a-5bd4bf36d85b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting trendwatch_shorts_pipeline.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile trendwatch_shorts_pipeline.py\n",
        "\"\"\"Trend‑Watching for Viral YouTube Shorts (robust v2)\n",
        "=====================================================\n",
        "A fully‑worked example that:\n",
        "* Queries the YouTube Data API v3 for Shorts published in the last *N* days\n",
        "* Downloads captions – or falls back to OCR on a handful of frames, or to Whisper audio transcription\n",
        "* Extracts key metadata & simple virality metrics (views/day, like ratio)\n",
        "* Performs lightweight topic/hook analysis with OpenAI’s GPT‑*‑mini\n",
        "* Saves a CSV ready for downstream MCP indexing (optional FastMCP server included)\n",
        "\n",
        "Key improvements over v1\n",
        "-----------------------\n",
        "* **Reliable media download** – uses `pytube` with graceful fall‑backs (age‑gate, 403s, missing streams)\n",
        "* **Modular fall‑back logic** – caption → frame‑OCR → Whisper audio, trying the cheapest first\n",
        "* **Progress visibility** – rich printouts + `tqdm` on API loops, per‑step success/fail logs\n",
        "* **Extra metrics** – `duration_sec`, `views_per_day`, `like_ratio`, `elapsed_days`, `virality_score`\n",
        "* **Configurable** – all knobs at the top; safe defaults; env‑var key loading\n",
        "* **Single‑file runnable** – python 3.10+, no notebook cells required\n",
        "\n",
        "Usage\n",
        "-----\n",
        "```bash\n",
        "!pip install --upgrade google-api-python-client youtube_transcript_api pytube moviepy fastmcp pillow openai pandas tqdm python-dotenv rich whisper @adzvire/pytubefix\n",
        "python trendwatch_shorts_pipeline.py --query \"AI tools\" --days 7 --out out.csv\n",
        "```\n",
        "(The `pytubefix` wheel is an actively maintained fork that patches YouTube’s rolling cipher changes.)\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "YOUTUBE_API_KEY = userdata.get('YOUTUBE_API_KEY')\n",
        "\n",
        "!python trendwatch_shorts_pipeline.py --query \"AI tools\" --days 7 --out out.csv --openai_key $OPENAI_API_KEY --yt_key $YOUTUBE_API_KEY\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import base64\n",
        "import datetime as dt\n",
        "import html\n",
        "import io\n",
        "import json\n",
        "import os\n",
        "import pathlib\n",
        "import sys\n",
        "import tempfile\n",
        "import textwrap\n",
        "from dataclasses import asdict, dataclass\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "import pandas as pd\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "from PIL import Image\n",
        "from pytube import YouTube\n",
        "from rich.console import Console\n",
        "from rich.progress import Progress\n",
        "from tqdm import tqdm\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "# Optional: a more resilient fork that keeps up with YouTube cipher changes\n",
        "try:\n",
        "    from pytubefix import YouTube as YTFix  # type: ignore\n",
        "except ImportError:\n",
        "    YTFix = None  # Fallback to stock pytube\n",
        "\n",
        "from openai import OpenAI\n",
        "from moviepy import VideoFileClip\n",
        "# ---------------------------------------------------------------------------\n",
        "# CONFIG\n",
        "# ---------------------------------------------------------------------------\n",
        "DEFAULT_QUERY = \"YouTube Shorts\"\n",
        "DEFAULT_DAYS_BACK = 10\n",
        "DEFAULT_MAX_RESULTS = 50\n",
        "MIN_VIEWS = 100_000\n",
        "LIKE_RATIO_THRESHOLD = 0.9\n",
        "FRAME_SAMPLES = 3 # limit o3 spendings\n",
        "VISION_MODEL = \"o3\"\n",
        "TEXT_MODEL = \"o4-mini\"\n",
        "\n",
        "console = Console()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Keys:\n",
        "    youtube: str\n",
        "    openai: str\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# UTILITIES\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def load_keys() -> Keys:\n",
        "    \"\"\"Load API keys from env or .env\"\"\"\n",
        "    try:\n",
        "        # from dotenv import load_dotenv\n",
        "        from google.colab import userdata\n",
        "        # load_dotenv()\n",
        "        yt = userdata.get(\"YOUTUBE_API_KEY\")\n",
        "        oa = userdata.get(\"OPENAI_API_KEY\")\n",
        "    except:\n",
        "        from dotenv import load_dotenv\n",
        "        load_dotenv()\n",
        "        yt = os.getenv(\"YOUTUBE_API_KEY\")\n",
        "        oa = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not yt or not oa:\n",
        "        console.print(\"[bold red]❌ Missing API keys – set YOUTUBE_API_KEY & OPENAI_API_KEY[\\n]\")\n",
        "        sys.exit(1)\n",
        "    return Keys(yt, oa)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# YOUTUBE HELPERS\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def yt_service(y_key: str):\n",
        "    return build(\"youtube\", \"v3\", developerKey=y_key)\n",
        "\n",
        "\n",
        "def search_shorts(yt, q: str, days_back: int, max_items: int) -> List[str]:\n",
        "    \"\"\"Return a list of video IDs for Shorts (<60 s) sorted by viewCount.\"\"\"\n",
        "    published_after = (\n",
        "        dt.datetime.utcnow() - dt.timedelta(days=days_back)\n",
        "    ).isoformat(\"T\") + \"Z\"\n",
        "    vids: List[str] = []\n",
        "    next_tok = None\n",
        "    while len(vids) < max_items:\n",
        "        resp = (\n",
        "            yt.search()\n",
        "            .list(\n",
        "                q=q,\n",
        "                type=\"video\",\n",
        "                videoDuration=\"short\",\n",
        "                part=\"id\",\n",
        "                maxResults=min(50, max_items - len(vids)),\n",
        "                publishedAfter=published_after,\n",
        "                order=\"viewCount\",\n",
        "                pageToken=next_tok,\n",
        "            )\n",
        "            .execute()\n",
        "        )\n",
        "        vids += [i[\"id\"][\"videoId\"] for i in resp[\"items\"]]\n",
        "        next_tok = resp.get(\"nextPageToken\")\n",
        "        if not next_tok:\n",
        "            break\n",
        "    return vids[:max_items]\n",
        "\n",
        "\n",
        "def fetch_details(yt, ids: List[str]) -> pd.DataFrame:\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "    for chunk in [ids[i : i + 50] for i in range(0, len(ids), 50)]:\n",
        "        data = (\n",
        "            yt.videos()\n",
        "            .list(id=\",\".join(chunk), part=\"snippet,statistics,contentDetails\")\n",
        "            .execute()\n",
        "        )\n",
        "        for item in data[\"items\"]:\n",
        "            stats = item.get(\"statistics\", {})\n",
        "            snip = item[\"snippet\"]\n",
        "            dur_iso = item[\"contentDetails\"][\"duration\"]  # e.g., PT58S\n",
        "            duration_sec = iso8601_duration_to_seconds(dur_iso)\n",
        "            publish_dt = dt.datetime.fromisoformat(snip[\"publishedAt\"].replace(\"Z\", \"+00:00\"))\n",
        "            now_utc      = dt.datetime.now(dt.timezone.utc)\n",
        "            elapsed_days = (now_utc - publish_dt).days or 1\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"video_id\": item[\"id\"],\n",
        "                    \"title\": snip[\"title\"],\n",
        "                    \"description\": snip.get(\"description\", \"\"),\n",
        "                    \"publish_dt\": publish_dt.isoformat(),\n",
        "                    \"channel\": snip.get(\"channelTitle\", \"\"),\n",
        "                    \"views\": int(stats.get(\"viewCount\", 0)),\n",
        "                    \"likes\": int(stats.get(\"likeCount\", 0)),\n",
        "                    \"comments\": int(stats.get(\"commentCount\", 0)),\n",
        "                    \"duration_sec\": duration_sec,\n",
        "                    \"elapsed_days\": elapsed_days,\n",
        "                    \"views_per_day\": int(stats.get(\"viewCount\", 0)) / elapsed_days,\n",
        "                }\n",
        "            )\n",
        "    df = pd.DataFrame(rows)\n",
        "    df[\"like_ratio\"] = df[\"likes\"] / (df[\"likes\"] + 1e-6)\n",
        "    return df\n",
        "\n",
        "\n",
        "def iso8601_duration_to_seconds(d: str) -> int:\n",
        "    \"\"\"Convert ISO 8601 duration string (e.g., PT58S) to seconds.\"\"\"\n",
        "    import re\n",
        "\n",
        "    m = re.match(r\"PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?\", d)\n",
        "    if not m:\n",
        "        return 0\n",
        "    h, m_, s = (int(x) if x else 0 for x in m.groups())\n",
        "    return h * 3600 + m_ * 60 + s\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# CAPTIONS / OCR / WHISPER PIPELINE\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def try_captions(video_id: str) -> Optional[str]:\n",
        "    \"\"\"Try YouTube transcripts first (no quota).\"\"\"\n",
        "    try:\n",
        "        tr = YouTubeTranscriptApi.get_transcript(\n",
        "            video_id, languages=[\"en\", \"en-US\", \"en-GB\"]\n",
        "        )\n",
        "        return \"\\n\".join(c[\"text\"] for c in tr)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def try_api_captions(yt, video_id: str) -> Optional[str]:\n",
        "    try:\n",
        "        caps = yt.captions().list(videoId=video_id, part=\"id\").execute()\n",
        "        if not caps[\"items\"]:\n",
        "            return None\n",
        "        track_id = caps[\"items\"][0][\"id\"]\n",
        "        body = yt.captions().download(id=track_id, tfmt=\"srt\").execute()[\"body\"]\n",
        "        return body\n",
        "    except HttpError:\n",
        "        return None\n",
        "\n",
        "\n",
        "def sample_frames(video_url: str, n: int = FRAME_SAMPLES) -> List[Image.Image]:\n",
        "    \"\"\"Download video (progressive mp4 if possible) and return n evenly‑spaced frames.\"\"\"\n",
        "    try:\n",
        "        yt_obj = (YTFix or YouTube)(video_url)\n",
        "        stream = yt_obj.streams.filter(progressive=True, file_extension=\"mp4\").first()\n",
        "        if not stream:\n",
        "            console.print(f\"[yellow]⚠️ No progressive stream for {video_url}\")\n",
        "            return []\n",
        "        tmp_path = stream.download(output_path=tempfile.gettempdir(), skip_existing=True)\n",
        "        clip = VideoFileClip(tmp_path)\n",
        "        dur = clip.duration\n",
        "        frames = [Image.fromarray(clip.get_frame(dur * (i + 1) / (n + 1))) for i in range(n)]\n",
        "        clip.close()\n",
        "        pathlib.Path(tmp_path).unlink(missing_ok=True)\n",
        "        return frames\n",
        "    except Exception as e:\n",
        "        console.print(f\"[yellow]⚠️ Frame sampling failed for {video_url}: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def ocr_frames(client: OpenAI, frames: List[Image.Image]) -> str:\n",
        "    texts: List[str] = []\n",
        "    for idx, img in enumerate(frames):\n",
        "        buf = io.BytesIO()\n",
        "        img.save(buf, format=\"PNG\")\n",
        "        b64 = base64.b64encode(buf.getvalue()).decode()\n",
        "        resp = client.chat.completions.create(\n",
        "            model=VISION_MODEL,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\"url\": f\"data:image/png;base64,{b64}\"},\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": \"Extract all visible text and a short scene description (≤40 words).\",\n",
        "                        },\n",
        "                    ],\n",
        "                }\n",
        "            ],\n",
        "            # max_completion_tokens=120,\n",
        "        )\n",
        "        texts.append(resp.choices[0].message.content.strip())\n",
        "    return \"\\n\".join(texts)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# NLP ANALYSIS\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def analyze_text(client: OpenAI, text: str) -> Dict[str, str]:\n",
        "    prompt = (\n",
        "        \"Read the captions & description below. Return two XML tags only:\\n\"\n",
        "        \"<topic> – main subject in ≤5 words\\n\"\n",
        "        \"<hooks> – concise list of virality hooks (≤40 chars each, ';'‑separated)\\n\\n\"\n",
        "        \"TEXT:\\n\" + text\n",
        "    )\n",
        "    resp = client.chat.completions.create(\n",
        "        model=TEXT_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        # max_tokens=120,\n",
        "    )\n",
        "    out = resp.choices[0].message.content\n",
        "    import re\n",
        "\n",
        "    topic = re.search(r\"<topic>(.*?)</topic>\", out, re.S)\n",
        "    hooks = re.search(r\"<hooks>(.*?)</hooks>\", out, re.S)\n",
        "    return {\n",
        "        \"topic\": html.unescape(topic.group(1).strip()) if topic else \"\",\n",
        "        \"hooks\": html.unescape(hooks.group(1).strip()) if hooks else \"\",\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# PIPELINE\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def process_video(\n",
        "    client: OpenAI,\n",
        "    yt,\n",
        "    vid: str,\n",
        "    frame_samples: int = FRAME_SAMPLES,\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"Return captions (from whichever source) + NLP analysis dict.\"\"\"\n",
        "    caption = try_captions(vid) or try_api_captions(yt, vid)\n",
        "    if not caption:\n",
        "        frames = sample_frames(f\"https://www.youtube.com/watch?v={vid}\", frame_samples)\n",
        "        if frames:\n",
        "            caption = ocr_frames(client, frames)\n",
        "    if not caption:\n",
        "        caption = \"\"\n",
        "    analysis = analyze_text(client, caption)\n",
        "    return {\"captions\": caption, **analysis}\n",
        "\n",
        "\n",
        "def virality_score(row: pd.Series) -> float:\n",
        "    return row.views / 1_000 + row.likes + row.views_per_day * 0.1\n",
        "\n",
        "\n",
        "def run_pipeline(OPENAI_API_KEY, YOUTUBE_API_KEY,\n",
        "                 query=DEFAULT_QUERY, days_back=DEFAULT_DAYS_BACK,\n",
        "                 max_results=DEFAULT_MAX_RESULTS, out_csv=\"trendwatch_results.csv\"):\n",
        "\n",
        "    # keys = load_keys()\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "    yt = yt_service(YOUTUBE_API_KEY)\n",
        "\n",
        "    console.print(f\"[bold cyan]🔍 Searching for shorts: '{query}' (last {days_back} days)…\")\n",
        "    vids = search_shorts(yt, query, days_back, max_results)\n",
        "    console.print(f\"Found {len(vids)} potential shorts – fetching details…\")\n",
        "    details = fetch_details(yt, vids)\n",
        "    console.print(\"Filtering by virality thresholds…\")\n",
        "    df = details[(details.views >= MIN_VIEWS) & (details.like_ratio >= LIKE_RATIO_THRESHOLD)].reset_index(drop=True)\n",
        "    console.print(f\"[green]✔ {len(df)} shorts pass the filter\")\n",
        "\n",
        "    captions = []\n",
        "    topics = []\n",
        "    hooks = []\n",
        "\n",
        "    with Progress() as progress:\n",
        "        task = progress.add_task(\"Analyzing\", total=len(df))\n",
        "        for row in df.itertuples():\n",
        "            pdata = process_video(client, yt, row.video_id)\n",
        "            captions.append(pdata[\"captions\"])\n",
        "            topics.append(pdata[\"topic\"])\n",
        "            hooks.append(pdata[\"hooks\"])\n",
        "            progress.advance(task)\n",
        "\n",
        "    df[\"captions\"] = captions\n",
        "    df[\"topic\"] = topics\n",
        "    df[\"catchy_factors\"] = hooks\n",
        "    df[\"virality_score\"] = df.apply(virality_score, axis=1)\n",
        "\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    console.print(f\"[bold green]✅ Saved results to {out_csv} ({len(df)} rows)\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# MCP SERVER (OPTIONAL)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def start_mcp(df: pd.DataFrame, host: str = \"0.0.0.0\", port: int = 8000):\n",
        "    from fastmcp import FastMCP\n",
        "\n",
        "    mcp = FastMCP(\n",
        "        name=\"YouTubeShortsTrendwatch\",\n",
        "        instructions=\"Trending YouTube Shorts corpus for deep research\",\n",
        "    )\n",
        "\n",
        "    @mcp.tool()\n",
        "    async def search(query: str) -> Dict[str, List[Dict[str, Any]]]:\n",
        "        sub = df[\n",
        "            df.title.str.contains(query, case=False, na=False)\n",
        "            | df.description.str.contains(query, case=False, na=False)\n",
        "        ]\n",
        "        return {\n",
        "            \"results\": [\n",
        "                {\n",
        "                    \"id\": r.video_id,\n",
        "                    \"title\": r.title,\n",
        "                    \"text\": textwrap.shorten(r.description, 140),\n",
        "                    \"url\": f\"https://www.youtube.com/watch?v={r.video_id}\",\n",
        "                }\n",
        "                for r in sub.itertuples()\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    @mcp.tool()\n",
        "    async def fetch(id: str) -> Dict[str, Any]:\n",
        "        r = df[df.video_id == id]\n",
        "        if r.empty:\n",
        "            raise ValueError(\"id not found\")\n",
        "        r = r.iloc[0]\n",
        "        return {\n",
        "            \"id\": id,\n",
        "            \"title\": r.title,\n",
        "            \"text\": f\"{r.description}\\n\\nCaptions:\\n{r.captions}\",\n",
        "            \"url\": f\"https://www.youtube.com/watch?v={id}\",\n",
        "            \"metadata\": {\n",
        "                \"publish_dt\": r.publish_dt,\n",
        "                \"views\": int(r.views),\n",
        "                \"likes\": int(r.likes),\n",
        "                \"virality_score\": float(r.virality_score),\n",
        "                \"topic\": r.topic,\n",
        "                \"catchy\": r.catchy_factors,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    console.print(f\"[cyan]🚀 Starting MCP server on {host}:{port} (SSE)…\")\n",
        "    mcp.run(transport=\"sse\", host=host, port=port)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# CLI\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def cli():\n",
        "    p = argparse.ArgumentParser(description=\"Trend‑watch YouTube Shorts\")\n",
        "    p.add_argument(\"--query\", default=DEFAULT_QUERY, help=\"Search query (default: 'YouTube Shorts')\")\n",
        "    p.add_argument(\"--days\", type=int, default=DEFAULT_DAYS_BACK, help=\"Published within last N days\")\n",
        "    p.add_argument(\"--max\", type=int, default=DEFAULT_MAX_RESULTS, help=\"Max shorts to fetch before filter\")\n",
        "    p.add_argument(\"--out\", default=\"trendwatch_results.csv\", help=\"Output CSV path\")\n",
        "    p.add_argument(\"--mcp\", action=\"store_true\", help=\"Launch an MCP server after collecting data\")\n",
        "    p.add_argument(\"--openai_key\", default=os.getenv(\"OPENAI_API_KEY\", \"\"), help=\"OpenAI API key\")\n",
        "    p.add_argument(\"--yt_key\", default=os.getenv(\"YOUTUBE_API_KEY\", \"\"), help=\"Youtube API key\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    df = run_pipeline(\n",
        "        OPENAI_API_KEY=args.openai_key,\n",
        "        YOUTUBE_API_KEY=args.yt_key,\n",
        "        query=args.query,\n",
        "        days_back=args.days,\n",
        "        max_results=args.max,\n",
        "        out_csv=args.out,\n",
        "    )\n",
        "    if args.mcp:\n",
        "        start_mcp(df)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cli()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-api-python-client youtube_transcript_api pytube moviepy fastmcp pillow openai pandas tqdm python-dotenv rich whisper pytubefix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UaoGOazl0P_",
        "outputId": "b5a90aa7-e0c6-40db-c259-af5292b5173e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.176.0)\n",
            "Requirement already satisfied: youtube_transcript_api in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pytube in /usr/local/lib/python3.11/dist-packages (15.0.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (2.2.1)\n",
            "Requirement already satisfied: fastmcp in /usr/local/lib/python3.11/dist-packages (2.10.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (14.0.0)\n",
            "Requirement already satisfied: whisper in /usr/local/lib/python3.11/dist-packages (1.1.10)\n",
            "Requirement already satisfied: pytubefix in /usr/local/lib/python3.11/dist-packages (9.3.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.25.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.2.0)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube_transcript_api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from youtube_transcript_api) (2.32.3)\n",
            "Requirement already satisfied: decorator<6.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.0.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: authlib>=1.5.2 in /usr/local/lib/python3.11/dist-packages (from fastmcp) (1.6.0)\n",
            "Requirement already satisfied: cyclopts>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from fastmcp) (3.22.2)\n",
            "Requirement already satisfied: exceptiongroup>=1.2.2 in /usr/local/lib/python3.11/dist-packages (from fastmcp) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from fastmcp) (0.28.1)\n",
            "Requirement already satisfied: mcp>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from fastmcp) (1.12.0)\n",
            "Requirement already satisfied: openapi-pydantic>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from fastmcp) (0.5.1)\n",
            "Requirement already satisfied: pydantic>=2.11.7 in /usr/local/lib/python3.11/dist-packages (from pydantic[email]>=2.11.7->fastmcp) (2.11.7)\n",
            "Requirement already satisfied: pyperclip>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from fastmcp) (1.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich) (2.19.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from whisper) (1.17.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from authlib>=1.5.2->fastmcp) (43.0.3)\n",
            "Requirement already satisfied: attrs>=23.1.0 in /usr/local/lib/python3.11/dist-packages (from cyclopts>=3.0.0->fastmcp) (25.3.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from cyclopts>=3.0.0->fastmcp) (0.16)\n",
            "Requirement already satisfied: rich-rst<2.0.0,>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from cyclopts>=3.0.0->fastmcp) (1.3.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->fastmcp) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->fastmcp) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->fastmcp) (0.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.10.0->fastmcp) (0.4.1)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.10.0->fastmcp) (4.24.0)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.10.0->fastmcp) (2.10.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.10.0->fastmcp) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.10.0->fastmcp) (2.4.1)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.10.0->fastmcp) (0.47.1)\n",
            "Requirement already satisfied: uvicorn>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.10.0->fastmcp) (0.35.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.7->pydantic[email]>=2.11.7->fastmcp) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.7->pydantic[email]>=2.11.7->fastmcp) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.7->pydantic[email]>=2.11.7->fastmcp) (0.4.1)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pydantic[email]>=2.11.7->fastmcp) (2.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->youtube_transcript_api) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->youtube_transcript_api) (2.4.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator>=2.0.0->pydantic[email]>=2.11.7->fastmcp) (2.7.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.20.0->mcp>=1.10.0->fastmcp) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.20.0->mcp>=1.10.0->fastmcp) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.20.0->mcp>=1.10.0->fastmcp) (0.26.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python3.11/dist-packages (from rich-rst<2.0.0,>=1.3.1->cyclopts>=3.0.0->fastmcp) (0.21.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.23.1->mcp>=1.10.0->fastmcp) (8.2.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography->authlib>=1.5.2->fastmcp) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography->authlib>=1.5.2->fastmcp) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "YOUTUBE_API_KEY = userdata.get('YOUTUBE_API_KEY')\n",
        "\n",
        "import datetime\n",
        "# date now\n",
        "data_now = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "tag = 'blonde'\n",
        "# tag = 'shorts'\n",
        "days_to_fetch = 3\n",
        "out_csv = f\"{tag}_{data_now}_{days_to_fetch}.csv\"\n",
        "!python trendwatch_shorts_pipeline.py --query $tag --days $days_to_fetch --out $out_csv --openai_key $OPENAI_API_KEY --yt_key $YOUTUBE_API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KBaUwrwl2Pm",
        "outputId": "12236368-8a5f-40aa-ff0b-0e441d898d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;36m🔍 Searching for shorts: \u001b[0m\u001b[1;36m'blonde'\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36mlast \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;36m days\u001b[0m\u001b[1;36m)\u001b[0m\u001b[1;36m…\u001b[0m\n",
            "Found \u001b[1;36m50\u001b[0m potential shorts – fetching details…\n",
            "Filtering by virality thresholds…\n",
            "\u001b[32m✔ \u001b[0m\u001b[1;32m4\u001b[0m\u001b[32m shorts pass the filter\u001b[0m\n",
            "\u001b[2KAnalyzing \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 25%\u001b[0m \u001b[36m-:--:--\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MHOkrCMyoEYz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}