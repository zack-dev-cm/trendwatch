{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNjE+Mn8vE5lgSmkXZutNj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zack-dev-cm/trendwatch/blob/main/trendwatch_yt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-OEn__4lhZ-",
        "outputId": "ea2ad9d8-1ded-4017-92c1-b5f5a76a8bb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing trendwatch_shorts_pipeline.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile trendwatch_shorts_pipeline.py\n",
        "\"\"\"Trend‑Watching for Viral YouTube Shorts (robust v2)\n",
        "=====================================================\n",
        "A fully‑worked example that:\n",
        "* Queries the YouTube Data API v3 for Shorts published in the last *N* days\n",
        "* Downloads captions – or falls back to OCR on a handful of frames, or to Whisper audio transcription\n",
        "* Extracts key metadata & simple virality metrics (views/day, like ratio)\n",
        "* Performs lightweight topic/hook analysis with OpenAI’s GPT‑4o‑mini\n",
        "* Saves a CSV ready for downstream MCP indexing (optional FastMCP server included)\n",
        "\n",
        "Key improvements over v1\n",
        "-----------------------\n",
        "* **Reliable media download** – uses `pytube` with graceful fall‑backs (age‑gate, 403s, missing streams)\n",
        "* **Modular fall‑back logic** – caption → frame‑OCR → Whisper audio, trying the cheapest first\n",
        "* **Progress visibility** – rich printouts + `tqdm` on API loops, per‑step success/fail logs\n",
        "* **Extra metrics** – `duration_sec`, `views_per_day`, `like_ratio`, `elapsed_days`, `virality_score`\n",
        "* **Configurable** – all knobs at the top; safe defaults; env‑var key loading\n",
        "* **Single‑file runnable** – python 3.10+, no notebook cells required\n",
        "\n",
        "Usage\n",
        "-----\n",
        "```bash\n",
        "!pip install --upgrade google-api-python-client youtube_transcript_api pytube moviepy fastmcp pillow openai pandas tqdm python-dotenv rich whisper @adzvire/pytubefix\n",
        "python trendwatch_shorts_pipeline.py --query \"AI tools\" --days 7 --out out.csv\n",
        "```\n",
        "(The `pytubefix` wheel is an actively maintained fork that patches YouTube’s rolling cipher changes.)\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "YOUTUBE_API_KEY = userdata.get('YOUTUBE_API_KEY')\n",
        "\n",
        "!python trendwatch_shorts_pipeline.py --query \"AI tools\" --days 7 --out out.csv --openai_key $OPENAI_API_KEY --yt_key $YOUTUBE_API_KEY\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import base64\n",
        "import datetime as dt\n",
        "import html\n",
        "import io\n",
        "import json\n",
        "import os\n",
        "import pathlib\n",
        "import sys\n",
        "import tempfile\n",
        "import textwrap\n",
        "from dataclasses import asdict, dataclass\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "import pandas as pd\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "from PIL import Image\n",
        "from pytube import YouTube\n",
        "from rich.console import Console\n",
        "from rich.progress import Progress\n",
        "from tqdm import tqdm\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "# Optional: a more resilient fork that keeps up with YouTube cipher changes\n",
        "try:\n",
        "    from pytubefix import YouTube as YTFix  # type: ignore\n",
        "except ImportError:\n",
        "    YTFix = None  # Fallback to stock pytube\n",
        "\n",
        "from openai import OpenAI\n",
        "from moviepy import VideoFileClip\n",
        "# ---------------------------------------------------------------------------\n",
        "# CONFIG\n",
        "# ---------------------------------------------------------------------------\n",
        "DEFAULT_QUERY = \"YouTube Shorts\"\n",
        "DEFAULT_DAYS_BACK = 10\n",
        "DEFAULT_MAX_RESULTS = 50\n",
        "MIN_VIEWS = 100_000\n",
        "LIKE_RATIO_THRESHOLD = 0.9\n",
        "FRAME_SAMPLES = 3 # limit o3 spendings\n",
        "VISION_MODEL = \"o3\"\n",
        "TEXT_MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "console = Console()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Keys:\n",
        "    youtube: str\n",
        "    openai: str\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# UTILITIES\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def load_keys() -> Keys:\n",
        "    \"\"\"Load API keys from env or .env\"\"\"\n",
        "    try:\n",
        "        # from dotenv import load_dotenv\n",
        "        from google.colab import userdata\n",
        "        # load_dotenv()\n",
        "        yt = userdata.get(\"YOUTUBE_API_KEY\")\n",
        "        oa = userdata.get(\"OPENAI_API_KEY\")\n",
        "    except:\n",
        "        from dotenv import load_dotenv\n",
        "        load_dotenv()\n",
        "        yt = os.getenv(\"YOUTUBE_API_KEY\")\n",
        "        oa = os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not yt or not oa:\n",
        "        console.print(\"[bold red]❌ Missing API keys – set YOUTUBE_API_KEY & OPENAI_API_KEY[\\n]\")\n",
        "        sys.exit(1)\n",
        "    return Keys(yt, oa)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# YOUTUBE HELPERS\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def yt_service(y_key: str):\n",
        "    return build(\"youtube\", \"v3\", developerKey=y_key)\n",
        "\n",
        "\n",
        "def search_shorts(yt, q: str, days_back: int, max_items: int) -> List[str]:\n",
        "    \"\"\"Return a list of video IDs for Shorts (<60 s) sorted by viewCount.\"\"\"\n",
        "    published_after = (\n",
        "        dt.datetime.utcnow() - dt.timedelta(days=days_back)\n",
        "    ).isoformat(\"T\") + \"Z\"\n",
        "    vids: List[str] = []\n",
        "    next_tok = None\n",
        "    while len(vids) < max_items:\n",
        "        resp = (\n",
        "            yt.search()\n",
        "            .list(\n",
        "                q=q,\n",
        "                type=\"video\",\n",
        "                videoDuration=\"short\",\n",
        "                part=\"id\",\n",
        "                maxResults=min(50, max_items - len(vids)),\n",
        "                publishedAfter=published_after,\n",
        "                order=\"viewCount\",\n",
        "                pageToken=next_tok,\n",
        "            )\n",
        "            .execute()\n",
        "        )\n",
        "        vids += [i[\"id\"][\"videoId\"] for i in resp[\"items\"]]\n",
        "        next_tok = resp.get(\"nextPageToken\")\n",
        "        if not next_tok:\n",
        "            break\n",
        "    return vids[:max_items]\n",
        "\n",
        "\n",
        "def fetch_details(yt, ids: List[str]) -> pd.DataFrame:\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "    for chunk in [ids[i : i + 50] for i in range(0, len(ids), 50)]:\n",
        "        data = (\n",
        "            yt.videos()\n",
        "            .list(id=\",\".join(chunk), part=\"snippet,statistics,contentDetails\")\n",
        "            .execute()\n",
        "        )\n",
        "        for item in data[\"items\"]:\n",
        "            stats = item.get(\"statistics\", {})\n",
        "            snip = item[\"snippet\"]\n",
        "            dur_iso = item[\"contentDetails\"][\"duration\"]  # e.g., PT58S\n",
        "            duration_sec = iso8601_duration_to_seconds(dur_iso)\n",
        "            publish_dt = dt.datetime.fromisoformat(snip[\"publishedAt\"].replace(\"Z\", \"+00:00\"))\n",
        "            now_utc      = dt.datetime.now(dt.timezone.utc)\n",
        "            elapsed_days = (now_utc - publish_dt).days or 1\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"video_id\": item[\"id\"],\n",
        "                    \"title\": snip[\"title\"],\n",
        "                    \"description\": snip.get(\"description\", \"\"),\n",
        "                    \"publish_dt\": publish_dt.isoformat(),\n",
        "                    \"channel\": snip.get(\"channelTitle\", \"\"),\n",
        "                    \"views\": int(stats.get(\"viewCount\", 0)),\n",
        "                    \"likes\": int(stats.get(\"likeCount\", 0)),\n",
        "                    \"comments\": int(stats.get(\"commentCount\", 0)),\n",
        "                    \"duration_sec\": duration_sec,\n",
        "                    \"elapsed_days\": elapsed_days,\n",
        "                    \"views_per_day\": int(stats.get(\"viewCount\", 0)) / elapsed_days,\n",
        "                }\n",
        "            )\n",
        "    df = pd.DataFrame(rows)\n",
        "    df[\"like_ratio\"] = df[\"likes\"] / (df[\"likes\"] + 1e-6)\n",
        "    return df\n",
        "\n",
        "\n",
        "def iso8601_duration_to_seconds(d: str) -> int:\n",
        "    \"\"\"Convert ISO 8601 duration string (e.g., PT58S) to seconds.\"\"\"\n",
        "    import re\n",
        "\n",
        "    m = re.match(r\"PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?\", d)\n",
        "    if not m:\n",
        "        return 0\n",
        "    h, m_, s = (int(x) if x else 0 for x in m.groups())\n",
        "    return h * 3600 + m_ * 60 + s\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# CAPTIONS / OCR / WHISPER PIPELINE\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def try_captions(video_id: str) -> Optional[str]:\n",
        "    \"\"\"Try YouTube transcripts first (no quota).\"\"\"\n",
        "    try:\n",
        "        tr = YouTubeTranscriptApi.get_transcript(\n",
        "            video_id, languages=[\"en\", \"en-US\", \"en-GB\"]\n",
        "        )\n",
        "        return \"\\n\".join(c[\"text\"] for c in tr)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def try_api_captions(yt, video_id: str) -> Optional[str]:\n",
        "    try:\n",
        "        caps = yt.captions().list(videoId=video_id, part=\"id\").execute()\n",
        "        if not caps[\"items\"]:\n",
        "            return None\n",
        "        track_id = caps[\"items\"][0][\"id\"]\n",
        "        body = yt.captions().download(id=track_id, tfmt=\"srt\").execute()[\"body\"]\n",
        "        return body\n",
        "    except HttpError:\n",
        "        return None\n",
        "\n",
        "\n",
        "def sample_frames(video_url: str, n: int = FRAME_SAMPLES) -> List[Image.Image]:\n",
        "    \"\"\"Download video (progressive mp4 if possible) and return n evenly‑spaced frames.\"\"\"\n",
        "    try:\n",
        "        yt_obj = (YTFix or YouTube)(video_url)\n",
        "        stream = yt_obj.streams.filter(progressive=True, file_extension=\"mp4\").first()\n",
        "        if not stream:\n",
        "            console.print(f\"[yellow]⚠️ No progressive stream for {video_url}\")\n",
        "            return []\n",
        "        tmp_path = stream.download(output_path=tempfile.gettempdir(), skip_existing=True)\n",
        "        clip = VideoFileClip(tmp_path)\n",
        "        dur = clip.duration\n",
        "        frames = [Image.fromarray(clip.get_frame(dur * (i + 1) / (n + 1))) for i in range(n)]\n",
        "        clip.close()\n",
        "        pathlib.Path(tmp_path).unlink(missing_ok=True)\n",
        "        return frames\n",
        "    except Exception as e:\n",
        "        console.print(f\"[yellow]⚠️ Frame sampling failed for {video_url}: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def ocr_frames(client: OpenAI, frames: List[Image.Image]) -> str:\n",
        "    texts: List[str] = []\n",
        "    for idx, img in enumerate(frames):\n",
        "        buf = io.BytesIO()\n",
        "        img.save(buf, format=\"PNG\")\n",
        "        b64 = base64.b64encode(buf.getvalue()).decode()\n",
        "        resp = client.chat.completions.create(\n",
        "            model=VISION_MODEL,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\"url\": f\"data:image/png;base64,{b64}\"},\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": \"Extract all visible text and a short scene description (≤40 words).\",\n",
        "                        },\n",
        "                    ],\n",
        "                }\n",
        "            ],\n",
        "            # max_completion_tokens=120,\n",
        "        )\n",
        "        texts.append(resp.choices[0].message.content.strip())\n",
        "    return \"\\n\".join(texts)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# NLP ANALYSIS\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def analyze_text(client: OpenAI, text: str) -> Dict[str, str]:\n",
        "    prompt = (\n",
        "        \"Read the captions & description below. Return two XML tags only:\\n\"\n",
        "        \"<topic> – main subject in ≤5 words\\n\"\n",
        "        \"<hooks> – concise list of virality hooks (≤40 chars each, ';'‑separated)\\n\\n\"\n",
        "        \"TEXT:\\n\" + text\n",
        "    )\n",
        "    resp = client.chat.completions.create(\n",
        "        model=TEXT_MODEL,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=120,\n",
        "    )\n",
        "    out = resp.choices[0].message.content\n",
        "    import re\n",
        "\n",
        "    topic = re.search(r\"<topic>(.*?)</topic>\", out, re.S)\n",
        "    hooks = re.search(r\"<hooks>(.*?)</hooks>\", out, re.S)\n",
        "    return {\n",
        "        \"topic\": html.unescape(topic.group(1).strip()) if topic else \"\",\n",
        "        \"hooks\": html.unescape(hooks.group(1).strip()) if hooks else \"\",\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# PIPELINE\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def process_video(\n",
        "    client: OpenAI,\n",
        "    yt,\n",
        "    vid: str,\n",
        "    frame_samples: int = FRAME_SAMPLES,\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"Return captions (from whichever source) + NLP analysis dict.\"\"\"\n",
        "    caption = try_captions(vid) or try_api_captions(yt, vid)\n",
        "    if not caption:\n",
        "        frames = sample_frames(f\"https://www.youtube.com/watch?v={vid}\", frame_samples)\n",
        "        if frames:\n",
        "            caption = ocr_frames(client, frames)\n",
        "    if not caption:\n",
        "        caption = \"\"\n",
        "    analysis = analyze_text(client, caption)\n",
        "    return {\"captions\": caption, **analysis}\n",
        "\n",
        "\n",
        "def virality_score(row: pd.Series) -> float:\n",
        "    return row.views / 1_000 + row.likes + row.views_per_day * 0.1\n",
        "\n",
        "\n",
        "def run_pipeline(OPENAI_API_KEY, YOUTUBE_API_KEY,\n",
        "                 query=DEFAULT_QUERY, days_back=DEFAULT_DAYS_BACK,\n",
        "                 max_results=DEFAULT_MAX_RESULTS, out_csv=\"trendwatch_results.csv\"):\n",
        "\n",
        "    # keys = load_keys()\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "    yt = yt_service(YOUTUBE_API_KEY)\n",
        "\n",
        "    console.print(f\"[bold cyan]🔍 Searching for shorts: '{query}' (last {days_back} days)…\")\n",
        "    vids = search_shorts(yt, query, days_back, max_results)\n",
        "    console.print(f\"Found {len(vids)} potential shorts – fetching details…\")\n",
        "    details = fetch_details(yt, vids)\n",
        "    console.print(\"Filtering by virality thresholds…\")\n",
        "    df = details[(details.views >= MIN_VIEWS) & (details.like_ratio >= LIKE_RATIO_THRESHOLD)].reset_index(drop=True)\n",
        "    console.print(f\"[green]✔ {len(df)} shorts pass the filter\")\n",
        "\n",
        "    captions = []\n",
        "    topics = []\n",
        "    hooks = []\n",
        "\n",
        "    with Progress() as progress:\n",
        "        task = progress.add_task(\"Analyzing\", total=len(df))\n",
        "        for row in df.itertuples():\n",
        "            pdata = process_video(client, yt, row.video_id)\n",
        "            captions.append(pdata[\"captions\"])\n",
        "            topics.append(pdata[\"topic\"])\n",
        "            hooks.append(pdata[\"hooks\"])\n",
        "            progress.advance(task)\n",
        "\n",
        "    df[\"captions\"] = captions\n",
        "    df[\"topic\"] = topics\n",
        "    df[\"catchy_factors\"] = hooks\n",
        "    df[\"virality_score\"] = df.apply(virality_score, axis=1)\n",
        "\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    console.print(f\"[bold green]✅ Saved results to {out_csv} ({len(df)} rows)\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# MCP SERVER (OPTIONAL)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def start_mcp(df: pd.DataFrame, host: str = \"0.0.0.0\", port: int = 8000):\n",
        "    from fastmcp import FastMCP\n",
        "\n",
        "    mcp = FastMCP(\n",
        "        name=\"YouTubeShortsTrendwatch\",\n",
        "        instructions=\"Trending YouTube Shorts corpus for deep research\",\n",
        "    )\n",
        "\n",
        "    @mcp.tool()\n",
        "    async def search(query: str) -> Dict[str, List[Dict[str, Any]]]:\n",
        "        sub = df[\n",
        "            df.title.str.contains(query, case=False, na=False)\n",
        "            | df.description.str.contains(query, case=False, na=False)\n",
        "        ]\n",
        "        return {\n",
        "            \"results\": [\n",
        "                {\n",
        "                    \"id\": r.video_id,\n",
        "                    \"title\": r.title,\n",
        "                    \"text\": textwrap.shorten(r.description, 140),\n",
        "                    \"url\": f\"https://www.youtube.com/watch?v={r.video_id}\",\n",
        "                }\n",
        "                for r in sub.itertuples()\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    @mcp.tool()\n",
        "    async def fetch(id: str) -> Dict[str, Any]:\n",
        "        r = df[df.video_id == id]\n",
        "        if r.empty:\n",
        "            raise ValueError(\"id not found\")\n",
        "        r = r.iloc[0]\n",
        "        return {\n",
        "            \"id\": id,\n",
        "            \"title\": r.title,\n",
        "            \"text\": f\"{r.description}\\n\\nCaptions:\\n{r.captions}\",\n",
        "            \"url\": f\"https://www.youtube.com/watch?v={id}\",\n",
        "            \"metadata\": {\n",
        "                \"publish_dt\": r.publish_dt,\n",
        "                \"views\": int(r.views),\n",
        "                \"likes\": int(r.likes),\n",
        "                \"virality_score\": float(r.virality_score),\n",
        "                \"topic\": r.topic,\n",
        "                \"catchy\": r.catchy_factors,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    console.print(f\"[cyan]🚀 Starting MCP server on {host}:{port} (SSE)…\")\n",
        "    mcp.run(transport=\"sse\", host=host, port=port)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# CLI\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def cli():\n",
        "    p = argparse.ArgumentParser(description=\"Trend‑watch YouTube Shorts\")\n",
        "    p.add_argument(\"--query\", default=DEFAULT_QUERY, help=\"Search query (default: 'YouTube Shorts')\")\n",
        "    p.add_argument(\"--days\", type=int, default=DEFAULT_DAYS_BACK, help=\"Published within last N days\")\n",
        "    p.add_argument(\"--max\", type=int, default=DEFAULT_MAX_RESULTS, help=\"Max shorts to fetch before filter\")\n",
        "    p.add_argument(\"--out\", default=\"trendwatch_results.csv\", help=\"Output CSV path\")\n",
        "    p.add_argument(\"--mcp\", action=\"store_true\", help=\"Launch an MCP server after collecting data\")\n",
        "    p.add_argument(\"--openai_key\", default=os.getenv(\"OPENAI_API_KEY\", \"\"), help=\"OpenAI API key\")\n",
        "    p.add_argument(\"--yt_key\", default=os.getenv(\"YOUTUBE_API_KEY\", \"\"), help=\"Youtube API key\")\n",
        "    args = p.parse_args()\n",
        "\n",
        "    df = run_pipeline(\n",
        "        OPENAI_API_KEY=args.openai_key,\n",
        "        YOUTUBE_API_KEY=args.yt_key,\n",
        "        query=args.query,\n",
        "        days_back=args.days,\n",
        "        max_results=args.max,\n",
        "        out_csv=args.out,\n",
        "    )\n",
        "    if args.mcp:\n",
        "        start_mcp(df)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    cli()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-api-python-client youtube_transcript_api pytube moviepy fastmcp pillow openai pandas tqdm python-dotenv rich whisper pytubefix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9UaoGOazl0P_",
        "outputId": "b67513eb-41a3-43f4-c707-528b086b16aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.176.0)\n",
            "Collecting youtube_transcript_api\n",
            "  Downloading youtube_transcript_api-1.1.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Collecting moviepy\n",
            "  Downloading moviepy-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting fastmcp\n",
            "  Downloading fastmcp-2.10.5-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Collecting pillow\n",
            "  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.96.1)\n",
            "Collecting openai\n",
            "  Downloading openai-1.97.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (13.9.4)\n",
            "Collecting rich\n",
            "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting whisper\n",
            "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytubefix\n",
            "  Downloading pytubefix-9.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.25.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.2.0)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube_transcript_api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from youtube_transcript_api) (2.32.3)\n",
            "Requirement already satisfied: decorator<6.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.0.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.1.12)\n",
            "Collecting authlib>=1.5.2 (from fastmcp)\n",
            "  Downloading authlib-1.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting cyclopts>=3.0.0 (from fastmcp)\n",
            "  Downloading cyclopts-3.22.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting exceptiongroup>=1.2.2 (from fastmcp)\n",
            "  Downloading exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from fastmcp) (0.28.1)\n",
            "Collecting mcp>=1.10.0 (from fastmcp)\n",
            "  Downloading mcp-1.12.0-py3-none-any.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openapi-pydantic>=0.5.1 (from fastmcp)\n",
            "  Downloading openapi_pydantic-0.5.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pydantic>=2.11.7 in /usr/local/lib/python3.11/dist-packages (from pydantic[email]>=2.11.7->fastmcp) (2.11.7)\n",
            "Requirement already satisfied: pyperclip>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from fastmcp) (1.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich) (2.19.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from whisper) (1.17.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from authlib>=1.5.2->fastmcp) (43.0.3)\n",
            "Requirement already satisfied: attrs>=23.1.0 in /usr/local/lib/python3.11/dist-packages (from cyclopts>=3.0.0->fastmcp) (25.3.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from cyclopts>=3.0.0->fastmcp) (0.16)\n",
            "Collecting rich-rst<2.0.0,>=1.3.1 (from cyclopts>=3.0.0->fastmcp)\n",
            "  Downloading rich_rst-1.3.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->fastmcp) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->fastmcp) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->fastmcp) (0.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
            "Collecting httpx-sse>=0.4 (from mcp>=1.10.0->fastmcp)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.10.0->fastmcp) (4.24.0)\n",
            "Collecting pydantic-settings>=2.5.2 (from mcp>=1.10.0->fastmcp)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.10.0->fastmcp) (0.0.20)\n",
            "Collecting sse-starlette>=1.6.1 (from mcp>=1.10.0->fastmcp)\n",
            "  Downloading sse_starlette-2.4.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.10.0->fastmcp) (0.47.1)\n",
            "Requirement already satisfied: uvicorn>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from mcp>=1.10.0->fastmcp) (0.35.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.7->pydantic[email]>=2.11.7->fastmcp) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.7->pydantic[email]>=2.11.7->fastmcp) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.7->pydantic[email]>=2.11.7->fastmcp) (0.4.1)\n",
            "Collecting email-validator>=2.0.0 (from pydantic[email]>=2.11.7->fastmcp)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->youtube_transcript_api) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->youtube_transcript_api) (2.4.0)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->pydantic[email]>=2.11.7->fastmcp)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.20.0->mcp>=1.10.0->fastmcp) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.20.0->mcp>=1.10.0->fastmcp) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.20.0->mcp>=1.10.0->fastmcp) (0.26.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python3.11/dist-packages (from rich-rst<2.0.0,>=1.3.1->cyclopts>=3.0.0->fastmcp) (0.21.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.23.1->mcp>=1.10.0->fastmcp) (8.2.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography->authlib>=1.5.2->fastmcp) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography->authlib>=1.5.2->fastmcp) (2.22)\n",
            "Downloading youtube_transcript_api-1.1.1-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.9/485.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading moviepy-2.2.1-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastmcp-2.10.5-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.97.0-py3-none-any.whl (764 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m765.0/765.0 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytubefix-9.3.0-py3-none-any.whl (760 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.2/760.2 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading authlib-1.6.0-py2.py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cyclopts-3.22.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
            "Downloading mcp-1.12.0-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.5/158.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openapi_pydantic-0.5.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich_rst-1.3.1-py3-none-any.whl (11 kB)\n",
            "Downloading sse_starlette-2.4.1-py3-none-any.whl (10 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: whisper\n",
            "  Building wheel for whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisper: filename=whisper-1.1.10-py3-none-any.whl size=41120 sha256=757d100525fda608ed4f12ae21e686baa9f4e3897298460bbea0ecba578881f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/65/ee/4e6672aabfa486d3341a39a04f8f87c77e5156149299b5a7d0\n",
            "Successfully built whisper\n",
            "Installing collected packages: whisper, pytubefix, pytube, python-dotenv, pillow, httpx-sse, exceptiongroup, dnspython, youtube_transcript_api, sse-starlette, rich, pandas, email-validator, rich-rst, pydantic-settings, openapi-pydantic, openai, moviepy, authlib, mcp, cyclopts, fastmcp\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.2.1\n",
            "    Uninstalling pillow-11.2.1:\n",
            "      Successfully uninstalled pillow-11.2.1\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.9.4\n",
            "    Uninstalling rich-13.9.4:\n",
            "      Successfully uninstalled rich-13.9.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.96.1\n",
            "    Uninstalling openai-1.96.1:\n",
            "      Successfully uninstalled openai-1.96.1\n",
            "  Attempting uninstall: moviepy\n",
            "    Found existing installation: moviepy 1.0.3\n",
            "    Uninstalling moviepy-1.0.3:\n",
            "      Successfully uninstalled moviepy-1.0.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "bigframes 2.10.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed authlib-1.6.0 cyclopts-3.22.2 dnspython-2.7.0 email-validator-2.2.0 exceptiongroup-1.3.0 fastmcp-2.10.5 httpx-sse-0.4.1 mcp-1.12.0 moviepy-2.2.1 openai-1.97.0 openapi-pydantic-0.5.1 pandas-2.3.1 pillow-11.3.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 pytube-15.0.0 pytubefix-9.3.0 rich-14.0.0 rich-rst-1.3.1 sse-starlette-2.4.1 whisper-1.1.10 youtube_transcript_api-1.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "1167e7eb45f54d3dbd8aad7bd109658c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "YOUTUBE_API_KEY = userdata.get('YOUTUBE_API_KEY')\n",
        "\n",
        "import datetime\n",
        "# date now\n",
        "data_now = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "tag = 'blonde'\n",
        "# tag = 'shorts'\n",
        "days_to_fetch = 3\n",
        "out_csv = f\"{tag}_{data_now}_{days_to_fetch}.csv\"\n",
        "!python trendwatch_shorts_pipeline.py --query $tag --days $days_to_fetch --out $out_csv --openai_key $OPENAI_API_KEY --yt_key $YOUTUBE_API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KBaUwrwl2Pm",
        "outputId": "0a28f630-fbb4-4129-f1b4-29146cf75257"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;36m🔍 Searching for shorts: \u001b[0m\u001b[1;36m'blonde'\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36mlast \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;36m days\u001b[0m\u001b[1;36m)\u001b[0m\u001b[1;36m…\u001b[0m\n",
            "Found \u001b[1;36m50\u001b[0m potential shorts – fetching details…\n",
            "Filtering by virality thresholds…\n",
            "\u001b[32m✔ \u001b[0m\u001b[1;32m4\u001b[0m\u001b[32m shorts pass the filter\u001b[0m\n",
            "\u001b[2KAnalyzing \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[1;32m✅ Saved results to blonde_2025-\u001b[0m\u001b[1;32m07\u001b[0m\u001b[1;32m-18_3.csv \u001b[0m\u001b[1;32m(\u001b[0m\u001b[1;32m4\u001b[0m\u001b[1;32m rows\u001b[0m\u001b[1;32m)\u001b[0m\n",
            "\u001b[1;36m🔍 Searching for shorts: \u001b[0m\u001b[1;36m'blonde'\u001b[0m\u001b[1;36m \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36mlast \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;36m days\u001b[0m\u001b[1;36m)\u001b[0m\u001b[1;36m…\u001b[0m\n",
            "Found \u001b[1;36m50\u001b[0m potential shorts – fetching details…\n",
            "Filtering by virality thresholds…\n",
            "\u001b[32m✔ \u001b[0m\u001b[1;32m4\u001b[0m\u001b[32m shorts pass the filter\u001b[0m\n",
            "\u001b[2KAnalyzing \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m"
          ]
        }
      ]
    }
  ]
}